let's dive back in with a fireside chat with two folks that should not need not need introduction from this crowd we'll have Greg Brockman chairman and CTO at open AI and Adam D'Angelo CEO of Cora and also a board member at open AI and they're going to talk about the future of AI from their perspective so please give a warm welcome to them thank you everyone so we have a lot to talk about today I wanted to just start off and Greg why don't you tell us about the history of how you got into AI as a field and and how opening I got started well I you know I think for me the the journey really started right before going to college and you know I was trying to figure out I how did how to spend my life working on mathematics and I had gotten really into chemistry in high school and I had actually started writing this chemistry textbook that I felt had this very mathematical bent to it and in order to to actually get this thing out into the world I decided to build a website and so I got I started to learn how to program and program was the thing that really just sucked me in because you know in math the thing I really loved was that you could think hard about a problem you'd understand it then you'd write dunst obscure way to call a program whereas with our they would call it proof and then with programming you do the same kind of thing but suddenly anyone could get the benefit of the thing that you you just thought about and discovered and that you can build a program that actually works for people and you know I just started to read more and more about the history of programming and how we got to this place and you start reading things like you know Alan Turing's 1950 paper on on the Turing test and in there you know he lays out this idea that look if we're gonna build machines that can actually go beyond having a human who has to sit and understand this entire domain an incredibly complex problem it wouldn't move beyond that to actually having machines that can you can interact with and it can solve problems that humans can't understand on their own that are outside of the grasp of human cognition you're going to need to go further you're going to have a machine that can actually be learned and I was so enamored with this idea and decided that was what I was gonna spend my life on now there's only one problem which is this was 2008 and nothing worked and so I actually started out at Harvard I transferred to MIT I made a much better choice but sadly I dropped out of MIT [Applause] because I really wanted to build things that had a real-world impact and the best thing I could see to do that was to do a start-up and you knows 2015 I'd been building this company called stripe which some people might have heard of for the past five years and build it from forward to about 250 people and it felt like there was something special happening in AI all right that it felt like every day you go on Hacker News you see a new story about deep learning does something and I was like what even is a deep learning and the more that I dug in the more that I realized that all of those dreams that I'd had in 2008 all those ideas that it's so captivating they were finally starting to work and I just wanted to help try to push them forward and make sure that they're going to actually benefit everyone great and so so open I I has come a really long way since since it started recently announced billion-dollar investment from from Microsoft so a good thing a lot of people are wondering is what is no that's it that's a massive amount of money for a research organization what is what is the organization gonna do with all of that I yeah I think it's a really important question and you know I think that first of all is some framing I think it's really interesting if you look at physics think about places like Cerner the you know think about like the LHC that the number of dollars that go into things like that it's actually really staggering you know it's like 20 billion dollars to build a massive particle accelerators these days and the thing about AI that's so amazing is that there's actually an ROI there right that we actually build these technologies that very directly can benefit people so I actually think it's actually really important that we a society step up the investment that we make into into AI and very specifically what we're going to be doing is that we're going to be building massive supercomputers that I think one of the biggest findings throughout the history of open AI has been that you know we tried a bunch of different things and what we found really works is scaling up our systems to unprecedented scale so so I think so just to be clear the most of the money is gonna go to the computer not to like pain researcher salaries researcher salaries are expensive but a computer turns out to be a lot more expensive right so so I think there's a kind of a common kind of like critique out there that there's I think a lot of people in academia might say there's a need for improvements in algorithms and and anything you have you can scale it up and do better but it seems like like open I was taking this unusual approach to to AI research and really focusing on scale how do you think about the the reasons for that so I think of what we're doing is you know you can look back we published this graph showing the amount of compute used for landmark AI results not just from opening AI but from the whole field the best results the most cutting-edge ones ones you've all heard of starting from 2012 and we found this incredibly smooth exponential which you know Moore's law was by comparison a measly snail-pace of doubling every 18 months the pace that we see in this field in practice with the past now seven years is 10x every single year that's five times faster than Moore's law and so you know we look at the the wave of progress and you can say well that's just a 2012 thing like how long can that really go for but if you actually zoom out and look at the entire history of this field and you look back to the very beginning that I there's actually number of people who have written about this that there's this professor named Moravec who've read about this stuff in the 90s that the thing that that really is tied with all of the major advances has been scaling up and having more computational power now recently recently this professor rich Sutton who is the inventor of reinforcement learning wrote a blog post called the bitter lesson and in the bitter lesson he says look that I the thing that we as a field have really valued and really strived for has been new ideas makes sense right this is the thing that you know think about what I n Stein did in physics right he sat down super you know super long and thought a lot and came up with a brilliant idea that revolutionized everything and so every AI researcher wants to do that but the point this post makes is that what action has worked has been scaling up further having you need to have a general method that can absorb that scale but we actually have methods like that and that the work that's required to make them work at 10x larger scale at 100x larger scale at a hundred thousand X larger scale is actually pretty small tweaks the same basic ideas and so what we see is that we view ourselves as really riding a wave of progress that's 60 years in the making and so I think that at the end of the day we view ourselves as pragmatist we didn't set out to try to build super large supercomputers when we started the company but we found that those are the results that actually really really you know I've moved the field forward so so recently you released this this really incredible language model called called GPT - and I think one of the things it was interesting about it it can generate incredible text in my opinion but the organization decided to take this approach in releasing it where we didn't share the model and and there was kind of an argument and a claim that the model is dangerous and it might not be good for the world for everyone have access to this model immediately which is a real break from from previous previous AI research so how did you think about that what was the rationale for that and and how do you think how do you think things are gonna change going forward so I think one thing we kind of realized over the course of opening eye has been that AI technologies are no longer toys right and I think that we're already in a place where and we can kind of see it in society that the impacts of these systems are pretty big right that you think about things from I you know just like recent recent years of systems affecting you the society that I you know I think it's become very clear that we need to not just be technologists building technology but we also need to be thinking about the impacts of these systems and now I think the question that's always been an open question is okay so research you know when does that start to become something that you actually have to think about impacts from day one or is it enough just to be like you know kind of on this cutting edge of science and just turning up dirt and then you know kind of letting letting everyone everyone who's productizing that be the ones who think through the implications and I think that we're close to that tipping point GPT 2 was the first time when we said you know we don't know whether or not this is something that we have to worry about right that you can imagine various use cases of you know using you know generating fake news at superhuman scale on how big of a deal is that we already have you know lots of you can go hire a farm of humans to to you know a bunch of humans ago and and in great fake news like if you have an AI that can do that is that a thing that changes the world in a negative way and we were genuinely uncertain within the company we had a bunch of debates that you know people made very good points in various directions and I think it's actually a pretty good policy that if you're uncertain if something you're not you're about to do is a bad idea that you have some caution and so that that's what we decide to do is err on that side but I think even more importantly we view this as an opportunity because the norms in this field have been towards total open publication even the idea of not releasing something is something that I think for a lot of people is this emotional shock right this idea that that you know because it's almost this this this feeling of like look I spend my whole life trying to develop new stuff and are you telling me that I might be doing something bad like and that's not really the statement we want to make right I think that it's really important to continue to make the progress because there's so much benefit there but at the same time we also need to take responsibility for what's coming next and so what we really wanted to do the opportunity here was to change the norms in the field to have a norm that yes you actually can go through a process and think about how to release something in stages or how to how to kind of bring people in incrementally rather than just blasting out to the world and so it generate a huge amount of controversy that you know I think that people you know one thing we learned was that just because you it was very nuanced message that I just said and it doesn't always come across in headlines and so people read the headlines and think that's exactly what when you said but the thing that I think is really encouraging to me is it actually worked we've actually seen a number of organizations replicate our results and also follow the same kind of stage release pattern that we have and so I think we've gone from a world where the idea of not releasing something was a you know fundamental violation and insult to science to a world where actually think about the implications of your work taking that seriously is supported so another another big result this year was opening at 5:00 can you talk a little about what that was and and why it was why you think it was important yeah so we spent about two years working on a system to play the very popular video game dota and there's a couple reasons we picked this game one is that it has a very large player base that are very very dedicated to being the you know peak of human performance at this game to give you a sense of scale it's like you know the the prize pool is the biggest of any eSports game it's about I think 40 million dollars each year goes to these professional eSports teams that all live together in these these these gamer houses to try to you know maximize their team team effort and it's a five-person games there's coordination and teamwork and strategy and that's what the game is about rather than trying to click super fast like other games and so you know we actually last year went to the the World Championships and played against some pros and we lost and and this year you know what we did is we just trained for longer and then we beat the world champions and the reason this is really exciting is twofold one you know we're not a gaming company that's not we're about we're not excited about sit you know just building AI agents and games for its own sake the thing we are excited us building general purpose technology that can apply to other problems right that what we really want is to increase the generality the systems we build and apply them to more and more problems in the world and so we actually could take the training system for this game and applied it to make a robotics breakthrough we're able to control a robot hand to perform a task that no one else could so one things I wonder and I heard other people wondering about is that in in playing a video game like dota it's it's there's the part of it this you gotta have a good strategy but there's another part of it that's like how fast you can click is actually a huge advantage and so you know I I feel like I could I could build some system to play Tetris better than any human and and it wouldn't really be impressive because it's just you know it's all about reaction time so how do you how do you respond to that what do you think about that that critique of the dota project yeah so I you know I kind of look at you know first of all the game it's if you can't rap with something like Starcraft and Starcraft the pros will literally measure their skill one of their metrics is how fast they can click the number of actions per minute in dota none of the pros track that because it's not actually that important for the game I a second thing I think is interesting is that when we lost last year against pros everyone said aha this shows that AI is not smart it can't strategize it can't you know solve these these important problems that humans do and then when we won you know then I you know people people then say oh well you could just click faster than people I kind of got to make up your mind but a third thing I think it's really fascinating is that the team that we'd be they were the world champions for the previous year so this year just a couple weeks ago in Shanghai became the world champion again they became the first team in dota history to ever win two championships much less in a row and the the way that they were playing was that they were being very aggressive and taking all these fights and that they were doing these early buybacks for all these in-game things that people used to say oh that's the only reason that the AI is winning is that it's able to do these things that no human would ever do and so in fact it turned out that the humans now actually play a lot more like what we saw from from our BOTS so I actually think that's you know at the end of the day that I think is what we're going for right that you want to have systems that can tell humans something they didn't know and that together that you can actually achieve things that would be impossible otherwise okay so I want to go on to another topic so opening has generated a lot of hype and media attention so through these things like GP to model being too dangerous to release through beating the the world champions at dota and I think there's a concern from some researchers that if if there's too much hype around the potential of AI and the promise of of general AI then that's gonna set an impossible standard and and then when the field doesn't reach that it's gonna lead to some kind of like AI winter like what happened in the past when when the things got too hyped up and and then the results didn't really deliver on the hype so how do you think about that do you think there's a risk of another AI winter do you think that we should be trying to like keep X vacations low so I think that's a really interesting question I think there's there's kind of have two angles on it the first angle is to ask you know what is different today and I think one thing that people do wrong actually is I think people don't think that hard about the history right we all have this picture in our head then we almost have these scars of the past of you know just like somehow there is over hype in the past and that you know bad things happened and we don't want that to happen again but if you actually go and read the histories the story is very interesting you know first of all the one thing that has really changed is the fact that the funding all used to be academic right used to be you know is like various funding institutions and really the whole reason deep learning was it was able to stay alive was that there was this Canadian moonshot organization that was willing to take a crazy bet on these these these these neural net people and it was really subject to the fashions of academia and kind of what was popular whereas today what drives AI research and what drives the funding is commercial value right you have companies like Google Facebook you have companies like Microsoft that are investing in building dollars in US and actually the day that Microsoft invested a billion dollars their stock price jumps by 10 billion dollars and you know I think that's because investors realize that yeah you make AI work it's going to generate huge amounts of value and so I think there's a flywheel that was never there before and so on that and that access I actually think that we're much more on show me the results and the results are what are going to continue to drive excitement and funding but the flipside is that you know it is the case that I look at the accelerating funding and the levels that we're talking about and I think where the investment should go you know again Mike we have it we have this this one investment but I actually think that we're still just a fly compared to what the the you know companies like Google are spending on this kind of research and if we actually can't get results you know if we can't actually deliver value then we actually shouldn't be funding it at accelerating exponential levels and so I think that it really comes down to it's really important that we are calibrated and that we're very honest about what we can do and what we can't do but I think of the thing that the field does wrong is to become very afraid of anyone getting excited about results because I think that there is a lot of exciting progress and the whole point of AI is to benefit people and so I think the more that we can actually tell that story make people realize why AI something they should be excited about it will make their lives better I think actually the better is going to be for everyone alright so we have time for one question from from the audience so someone asked if if opening AI is you know is sort of branded and as a mission around openness how does that fit with not releasing your your work yeah I think this is also a very important question right so when we started open the I we had a goal of trying to make sure that general intelligence you know the most advanced AI systems we can build would benefit everyone like that's what we wanted to do and we had a plan you know our plan was to involve everyone in the development process of this technology and that felt like a really good way to make sure the benefits go to everyone you know I think it works super well for the internet works super well for you think about the impact of open-source software it's a really good strategy that has I think really transformed the world in that domain the thing that really happened is over the past couple years that we've realized that these technologies you know that they have dual use implications right that you can use any piece of AI for something great but the someone could else could repurpose it for a negative application and we get asked all the time ok you're going to build a super powerful technology how are you going to make sure that people don't use it for Bad ends how you're going to make sure it doesn't fall into the hands of malicious actors and I think that there's really only one answer for that that whoever is developing this technology I think has a responsibility to make sure it's being kept safe and secure and so you know I think that the shift in strategy for us has been to focus not on including everyone in the development process so I think the more that that can happen the better it is because you get all these all these beneficial effects on but to instead focus as our core strategy on ensuring that everyone participates in the benefits and so you look at our corporate structure you know we start as a non-profit we changed to a structure we call a capped profit and some of the details you know they're the whole reason that we did this thing is to ensure that we can actually execute on the strategy so if we succeed that the benefits aren't locked up in one institution and so I think that's that's that's a lot of how we think about it great so thank you everyone and thanks Greg thank you [Applause] you